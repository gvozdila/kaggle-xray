{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open('alexnet.fea', 'rb') as f:\n",
    "    alex_features = pickle.load(f)\n",
    "\n",
    "train_metadata_df = pd.read_csv('dicom_metadata_train_merged.csv') # Took it from kaggle notebooks\n",
    "import torch\n",
    "\n",
    "alex_np_fea = [ elem.numpy()[0] for elem in alex_features ]\n",
    "\n",
    "\n",
    "mult_lab = train_metadata_df.iloc[:, 64:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "#from torchvision.io import read_image toooo old fot that stuff\n",
    "\n",
    "\n",
    "\n",
    "class featuresDataset(Dataset):\n",
    "    def __init__(self, merged_df, features_list, transform = None, target_transform = None):\n",
    "        self.img_labels = merged_df\n",
    "        self.features_list = features_list\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        print(\"Dataset loaded. Length = \",len(self.features_list))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features_list[idx]\n",
    "        \n",
    "        label = self.img_labels.iloc[idx, 64:]\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return feature, label\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        return self.features_list , self.img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "\n",
    "training_data = featuresDataset(\n",
    "    merged_df=train_metadata_df,\n",
    "    features_list=alex_features,\n",
    "#    train=True,\n",
    "#    download=True,\n",
    "    #transform=ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[55]\n",
    "type(training_data.get_all_data()[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(training_data))\n",
    "test_size = len(training_data) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(training_data, [train_size, test_size])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(alex_np_fea, mult_lab, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example cant work with multi-lable.\n",
    "# Need to find other solution\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif,chi2\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "def select_features(X_train, y_train, X_test,i):\n",
    "    # configure to select all features\n",
    "    #fs = SelectKBest(score_func=f_classif, k=i)\n",
    "    fs = SelectKBest(score_func=f_classif, k=i)\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs\n",
    "\n",
    "\n",
    "#fs_X_train,fs_X_test = select_features(X_train, y_train, X_test,  250 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif\n",
    "import numpy as np\n",
    "\n",
    "threshold = 200.5\n",
    "\n",
    "\n",
    "selected_features = [] \n",
    "for label in y_train.columns:\n",
    "    selector = SelectKBest(f_classif, k='all')\n",
    "    selector.fit(X_train, y_train[label])\n",
    "    selected_features.append(list(selector.scores_))\n",
    "\n",
    "#/ / MeanCS \n",
    "#selected_features = np.mean(selected_features, axis=0) > threshold\n",
    "#// MaxCS\n",
    "selected_features = np.max(selected_features, axis=0) > threshold\n",
    "\n",
    "# Этот вариант работает с multi-label. Но тут есть тонкость.\n",
    "# Нужно отобрать features так, чтоб для каждого класса самые информативные присутствовали. \n",
    "# А мне кажется, что присутствуют те, в которых большая информативность хотя бы для одного класса.\n",
    "# И возможна ситуация, что найдется класс, для которого не отобрались хоть сколько-то информативные фичи.\n",
    "\n",
    "#len(selected_features[0])\n",
    "selected_features\n",
    "len(X_train[0][selected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VarianceThreshold is a simple baseline approach to feature selection. \n",
    "# It removes all features whose variance doesn’t meet some threshold\n",
    "\n",
    "# Can we use it?\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tr = np.arange(0.0, 1.0, 0.01)\n",
    "s=[]\n",
    "for ttt in tr:\n",
    "    selector = VarianceThreshold(threshold=ttt)\n",
    "    selector.fit_transform(X_train).shape\n",
    "    \n",
    "    s.append(selector.transform(X_test[0:1]).shape[1])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(tr, s)\n",
    "\n",
    "ax.set(xlabel='Threshold', ylabel='Feature remains',\n",
    "       title='About as simple as it gets, folks')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feature_names = [f\"feature {i}\" for i in range(X_train[0].shape[0])]\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation_importance \n",
    "# Permutation feature importance is a model inspection technique that\n",
    "#  can be used for any fitted estimator when the data is tabular.\n",
    "# This is especially useful for non-linear or opaque estimators. \n",
    "# The permutation feature importance is defined to be the decrease\n",
    "#  in a model score when a single feature value is randomly shuffled 1. \n",
    "# This procedure breaks the relationship between the feature and the target,\n",
    "#  thus the drop in the model score is indicative of how much the model depends\n",
    "#  on the feature. This technique benefits from being model agnostic and\n",
    "#  can be calculated many times with different permutations of the feature.\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "start_time = time.time()\n",
    "result = permutation_importance(\n",
    "    forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "len(X_train), X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=7, n_estimators= 100000)\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "(rfc.predict(X_train) - y_train).mean()\n",
    "(rfc.predict(X_test) - y_test).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(rfc, open(filename, 'wb'))\n",
    " \n",
    "# load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
