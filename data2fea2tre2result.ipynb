{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "\n",
    "import pickle as pk # to save list as file\n",
    "import joblib # Another way to sale/load model and data\n",
    "\n",
    "import gc # garbage collector, can help you free memory\n",
    "!pip install import_ipynb\n",
    "import import_ipynb # tool allow you to import from .ipynb files. Usualy import only from .py files.\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "\n",
    "# torch can use CUDA, this line check if CUDA is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import DicomDataset, DicomTestDataset # my dataset class\n",
    "from create_dataset import train_metadata_df # import dataset csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instance of DicomDataset\n",
    "from torchvision.transforms import ToTensor, ToPILImage, Compose\n",
    "from torchvision.transforms import RandAugment # makes troubles with dataloader\n",
    "from torchvision.transforms import AutoAugmentPolicy, AutoAugment\n",
    "\n",
    "from torchvision.transforms import RandomRotation, RandomPerspective, RandomAffine, RandomAdjustSharpness\n",
    "\n",
    "rand_aug_transformation = Compose([\n",
    "    ToTensor(),\n",
    "    #AutoAugment(AutoAugmentPolicy.IMAGENET)\n",
    "    #RandomRotation(degrees=(-90,90)),\n",
    "    RandomAffine(degrees=(-180,180),scale = (0.8, 1.2), translate = (0.1, 0.1)  ),\n",
    "    RandomAdjustSharpness(sharpness_factor=2),\n",
    "    RandomPerspective(distortion_scale = 0.2, p = 0.3) \n",
    "    ])\n",
    "\n",
    "# policies = [T.AutoAugmentPolicy.CIFAR10, T.AutoAugmentPolicy.IMAGENET, T.AutoAugmentPolicy.SVHN]\n",
    "# augmenters = [T.AutoAugment(policy) for policy in policies]\n",
    "# imgs = [\n",
    "#     [augmenter(orig_img) for _ in range(4)]\n",
    "#     for augmenter in augmenters\n",
    "# ]\n",
    "\n",
    "train_dataset = DicomDataset(\n",
    "    merged_df=train_metadata_df,\n",
    "    image_dir=\"train\",\n",
    "#    train=True,\n",
    "#    download=True,\n",
    "    preload = True,\n",
    "    transform=rand_aug_transformation\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# test_dataset = DicomTestDataset(\n",
    "#     image_dir=\"test\",\n",
    "# #    train=True,\n",
    "# #    download=True,\n",
    "#     preload = False,\n",
    "#     transform=ToTensor()\n",
    "# )\n",
    "\n",
    "# test_augmented_dataset = DicomTestDataset(\n",
    "#     image_dir=\"test\",\n",
    "# #    train=True,\n",
    "# #    download=True,\n",
    "#     preload = False,\n",
    "#     transform=rand_aug_transformation\n",
    "# )\n",
    "\n",
    "# >>> transforms.Compose([\n",
    "# >>>     transforms.CenterCrop(10),\n",
    "# >>>     transforms.PILToTensor(),\n",
    "# >>>     transforms.ConvertImageDtype(torch.float),\n",
    "# >>> ])\n",
    "#augmenter = T.RandAugment()\n",
    "\n",
    "\n",
    "# train_size = int(0.8 * len(training_data))\n",
    "# test_size = len(training_data) - train_size\n",
    "# train_part, test_part = torch.utils.data.random_split(training_data, [train_size, test_size])\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "# batch_size=32 ok for alexnet&resnet. Too big for densenet. let's try smaller size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "# test_augmented_dataloader = DataLoader(test_augmented_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_dataloader.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for list_of_tensor_images,list_of_labels in train_dataloader:\n",
    "#     print(list_of_tensor_images.shape,\"LABELS !!!!\",list_of_labels)\n",
    "#     break\n",
    "\n",
    "# for list_of_tensor_images,list_of_labels in test_dataloader:\n",
    "#     print(list_of_tensor_images.shape,\"LABELS !!!!\",list_of_labels)\n",
    "#     break\n",
    "\n",
    "# for list_of_tensor_images,list_of_labels in test_augmented_dataloader:\n",
    "#     print(list_of_tensor_images.shape,\"LABELS !!!!\",list_of_labels)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models.alexnet(pretrained=True)\n",
    "# fit should not be used it cycle. This fuction needs more work.\n",
    "def train_trees_with_nn(tree_name,nn_name, n_epoch = 10,prefix = '', suffix = ''):\n",
    "    print(f'Starting {nn_name} for {tree_name}\\n')\n",
    "    tree_filename = prefix + tree_name +\"_\"+ nn_name + suffix + \".tre\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    #model = model_dict[model_name]\n",
    "    model = create_model(nn_name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    ds_res= []\n",
    "    tree = create_tree(tree_name,n_estimators= 100000)\n",
    "    if (exists(tree_filename) ):\n",
    "        with open(tree_filename, 'rb') as f:\n",
    "            tree = Pickle.load(f)\n",
    "\n",
    "    #print(\"start \",model_name)\n",
    "    #for image,lable in data_set:\n",
    "    for epoch in range(n_epoch):\n",
    "        print(f\"Starting epoch ={epoch}\\n\")\n",
    "        \n",
    "        for list_of_tensor_images,list_of_labels in (train_dataloader):\n",
    "            print(\"And \",len(list_of_labels),\" items more, baby\")\n",
    "            torch.cuda.empty_cache()\n",
    "            with torch.no_grad():\n",
    "                features = model(list_of_tensor_images.to(device))\n",
    "                list_of_tensor_images.cpu()\n",
    "                del list_of_tensor_images\n",
    "                #features = model(torch.unsqueeze(image,0).to(device))\n",
    "                #print(type(features),type(features.cpu()),features.cpu().shape,len([ elem.cpu() for elem in features ]),features.shape)\n",
    "\n",
    "            \n",
    "            #alex_np_fea = [ elem.numpy()[0] for elem in alex_features ]\n",
    "            sk_feat = [ elem.numpy() for elem in features.cpu() ]\n",
    "            #print(len(sk_feat),len(sk_feat[0]),sk_feat)\n",
    "            #print(len(list_of_labels),len(list_of_labels[0]),list_of_labels)\n",
    "            \n",
    "            tree.fit(sk_feat,list_of_labels)\n",
    "            #tree_pred = tree.predict(sk_feat)\n",
    "            #print(hamming_loss(tree_pred,list_of_labels) ) # Boolshit, test on same data as train . But i need to see if tree touches all data\n",
    "            #ds_res.append(im_res.to('cpu'))\n",
    "            del features\n",
    "            del sk_feat\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        with open(tree_filename, 'wb') as f: # Save tree data every epoch\n",
    "            Pickle.dump(tree, f)\n",
    "    del model\n",
    "    del tree\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"alexnet\",\"resnet50\", \n",
    "\"densenet161\", \n",
    "\"wide_resnet50_2\",\n",
    "\"vgg13\"]\n",
    "tree_names = [\"RandomForest\",\n",
    "            \"ExtraTrees\"]\n",
    "\n",
    "def create_model(model_name):\n",
    "    model_dict = {\"alexnet\":models.alexnet(pretrained=True),\"resnet50\":models.resnet50(pretrained=True), \n",
    "    \"densenet161\":models.densenet161(pretrained=True), \n",
    "    \"wide_resnet50_2\":models.wide_resnet50_2(pretrained=True),\n",
    "    \"vgg13\":models.vgg13(pretrained=True)}\n",
    "    return model_dict[model_name]\n",
    "    \n",
    "\n",
    "\n",
    "def create_tree(tree_name, n_estimators = 10000):\n",
    "    trees_ensembles = { \"RandomForest\":RandomForestClassifier(n_jobs=8,verbose=1, n_estimators= n_estimators),\n",
    "            \"ExtraTrees\":ExtraTreesClassifier(n_jobs=8,verbose=1, n_estimators= n_estimators)}\n",
    "    return trees_ensembles[tree_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_trees_with_nn_in_cache(tree_name,nn_name, n_epoch = 10,prefix = '', suffix = ''):\n",
    "    print(f'Starting {nn_name} for {tree_name}\\n')\n",
    "    tree_filename = prefix + tree_name +\"_\"+ nn_name + suffix + \".tre\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    #model = model_dict[model_name]\n",
    "    model = create_model(nn_name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    #final_in_features = model.fc.in_features\n",
    "    #model.fc = nn.Linear(final_in_features, 200)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    tree = create_tree(tree_name,n_estimators= 20000)\n",
    "    if (exists(tree_filename) ):\n",
    "        tree =joblib.load(tree_filename) # joblib way\n",
    "\n",
    "        # with open(tree_filename, 'rb') as f: # pickle way\n",
    "        #     tree = Pickle.load(f)\n",
    "    ds_res= []\n",
    "    ds_lab = []\n",
    "    #print(\"start \",model_name)\n",
    "    #for image,lable in data_set:\n",
    "    for epoch in range(n_epoch):\n",
    "        print(f\"Starting epoch ={epoch}\\n\")\n",
    "        \n",
    "        batch_counter = 0\n",
    "        for list_of_tensor_images,list_of_labels in (train_dataloader):\n",
    "            print(\"And \",len(list_of_labels),\" items more, baby. Batch number \",batch_counter)\n",
    "            batch_counter += 1\n",
    "            torch.cuda.empty_cache()\n",
    "            with torch.no_grad():\n",
    "                features = model(list_of_tensor_images.to(device))\n",
    "                list_of_tensor_images.cpu()\n",
    "                del list_of_tensor_images\n",
    "                #features = model(torch.unsqueeze(image,0).to(device))\n",
    "                #print(type(features),type(features.cpu()),features.cpu().shape,len([ elem.cpu() for elem in features ]),features.shape)\n",
    "\n",
    "            \n",
    "            #alex_np_fea = [ elem.numpy()[0] for elem in alex_features ]\n",
    "            sk_feat = [ elem.numpy() for elem in features.cpu() ]\n",
    "            sk_labs = [ elem.numpy() for elem in list_of_labels.cpu() ]\n",
    "            ds_res.extend(sk_feat)\n",
    "            ds_lab.extend(sk_labs)\n",
    "            #print(len(sk_feat),len(sk_feat[0]),sk_feat)\n",
    "            #print(len(list_of_labels),len(list_of_labels[0]),list_of_labels)\n",
    "            \n",
    "            #tree.fit(sk_feat,list_of_labels)\n",
    "            #tree_pred = tree.predict(sk_feat)\n",
    "            #print(hamming_loss(tree_pred,list_of_labels) ) # Boolshit, test on same data as train . But i need to see if tree touches all data\n",
    "            #ds_res.append(im_res.to('cpu'))\n",
    "\n",
    "            #print(ds_res,ds_lab)\n",
    "            #tree.fit(ds_res,ds_lab)\n",
    "            del features\n",
    "            del sk_feat\n",
    "            del sk_labs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        #print(f\"Teaching {tree_name} after epoch {epoch} with features from {nn_name}\")\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(ds_res, ds_lab, test_size=0.33, random_state=42)\n",
    "        #tree.fit(X_train,y_train)\n",
    "        #tree_pred = tree.predict(X_test)\n",
    "        #print(\"CUrrent hamming loss is \",hamming_loss(tree_pred,y_test) )\n",
    "\n",
    "    print(\"final education\")\n",
    "    tree.fit(ds_res,ds_lab)\n",
    "    del ds_res\n",
    "    del ds_lab\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    joblib.dump(tree, tree_filename, compress = 3)\n",
    "    # with open(tree_filename, 'wb') as f: # pickle way\n",
    "    #     Pickle.dump(tree, f)            \n",
    "        \n",
    "    del model\n",
    "    del tree\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in model_names:\n",
    "#     for tree in tree_names:\n",
    "#         train_trees_with_nn_in_cache(tree,model, n_epoch = 20,prefix = '', suffix = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Starting densenet161 features preparing -- cannot use batch size = batch_size=32\n",
    "\n",
    "def save_augmented_nn(nn_name, n_epoch = 10,prefix = '', suffix = ''):\n",
    "    print(f'Starting {nn_name} features preparing\\n')\n",
    "    #tree_filename = prefix + tree_name +\"_\"+ nn_name + suffix + \".tre\"\n",
    "    features_filename = prefix + nn_name + suffix + \".fea\"\n",
    "    labels_filename = prefix + nn_name + suffix + \".labels\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    #model = model_dict[model_name]\n",
    "    model = create_model(nn_name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    ds_res= []\n",
    "    ds_lab = []\n",
    "    #print(\"start \",model_name)\n",
    "    #for image,lable in data_set:\n",
    "    for epoch in range(n_epoch):\n",
    "        print(f\"Starting epoch ={epoch}\\n\")\n",
    "        \n",
    "        batch_counter = 0\n",
    "        for list_of_tensor_images,list_of_labels in (train_dataloader):\n",
    "            print(\"And \",len(list_of_labels),\" items more, baby. Batch number \",batch_counter)\n",
    "            batch_counter += 1\n",
    "            torch.cuda.empty_cache()\n",
    "            with torch.no_grad():\n",
    "                features = model(list_of_tensor_images.to(device))\n",
    "                list_of_tensor_images.cpu()\n",
    "                del list_of_tensor_images\n",
    "                #features = model(torch.unsqueeze(image,0).to(device))\n",
    "                #print(type(features),type(features.cpu()),features.cpu().shape,len([ elem.cpu() for elem in features ]),features.shape)\n",
    "\n",
    "            \n",
    "            #alex_np_fea = [ elem.numpy()[0] for elem in alex_features ]\n",
    "            sk_feat = [ elem.numpy() for elem in features.cpu() ]\n",
    "            sk_labs = [ elem.numpy() for elem in list_of_labels.cpu() ]\n",
    "            ds_res.extend(sk_feat)\n",
    "            ds_lab.extend(sk_labs)\n",
    "            #print(len(sk_feat),len(sk_feat[0]),sk_feat)\n",
    "            \n",
    "            del features\n",
    "            del sk_feat\n",
    "            del sk_labs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "    print(f\"Saving features and labels to files {features_filename} and {labels_filename}\")\n",
    "    joblib.dump(ds_res, features_filename, compress = 3)\n",
    "    joblib.dump(ds_lab, labels_filename, compress = 3)\n",
    "    del ds_res\n",
    "    del ds_lab\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "        \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# model_names = [\"alexnet\",\"resnet50\", \n",
    "# \"densenet161\", \n",
    "# \"wide_resnet50_2\",\n",
    "# \"vgg13\"]\n",
    "\n",
    "\n",
    "# removed alex & resnet. \n",
    "model_names = [\n",
    "\"densenet161\", \n",
    "\"wide_resnet50_2\",\n",
    "\"vgg13\"]\n",
    "\n",
    "recalc_features = True\n",
    "if recalc_features:\n",
    "    for model in model_names:\n",
    "        save_augmented_nn(model, n_epoch = 20,prefix = '', suffix = '')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cPickle\n",
    "\n",
    "# rf = RandomForestRegresor()\n",
    "# rf.fit(X, y)\n",
    "\n",
    "# with open('path/to/file', 'wb') as f:\n",
    "#     cPickle.dump(rf, f)\n",
    "\n",
    "\n",
    "# # in your prediction file                                                                                                                                                                                                           \n",
    "\n",
    "# with open('path/to/file', 'rb') as f:\n",
    "#     rf = cPickle.load(f)\n",
    "\n",
    "\n",
    "# preds = rf.predict(new_X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
